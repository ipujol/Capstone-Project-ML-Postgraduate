{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T19:59:04.251237Z",
     "start_time": "2023-07-03T19:59:02.699163Z"
    },
    "id": "mEVxznqqxSdc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fernadavo/.virtualenvs/dev_new/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/fernadavo/.virtualenvs/dev_new/lib/python3.7/site-packages/ipykernel_launcher.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import requests\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Data in Validation, Control and Test Groups and perform the encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T19:59:45.121893Z",
     "start_time": "2023-07-03T19:59:18.601328Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_data = pd.read_csv('clean_data_all_years.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T19:59:46.150870Z",
     "start_time": "2023-07-03T19:59:45.124734Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random split using all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T18:26:35.733652Z",
     "start_time": "2023-07-03T18:25:35.073927Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fernadavo/.virtualenvs/dev_new/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "/Users/fernadavo/.virtualenvs/dev_new/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/fernadavo/.virtualenvs/dev_new/lib/python3.7/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/fernadavo/.virtualenvs/dev_new/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "#!pip install category_encoders\n",
    "import category_encoders as ce\n",
    "\n",
    "X = clean_data.drop('percentage_docks_available', axis=1)  # Replace 'target_variable' with your actual target column name\n",
    "y = clean_data[['percentage_docks_available']]\n",
    "              \n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "X_control, X_validation, y_control, y_validation = train_test_split(X_train_val, y_train_val, test_size=0.1/0.9, random_state=42)\n",
    "\n",
    "target_encoder = ce.TargetEncoder(cols=['station_id'])\n",
    "X_control['station_encoded'] = target_encoder.fit_transform(X_control['station_id'], y_control)\n",
    "\n",
    "encoding_station_dict = X_control.groupby(['station_id', 'station_encoded']).mean().reset_index()\\\n",
    "[['station_id', 'station_encoded']].set_index('station_id')['station_encoded'].to_dict()\n",
    "\n",
    "X_control = X_control[['station_encoded', 'precipitation', 'temperature_2m', 'altitude', 'capacity',\n",
    "                         'month', 'day', 'hour','CTX-1', 'CTX-2', 'CTX-3', 'CTX-4']]\n",
    "\n",
    "X_validation['station_encoded'] = X_validation['station_id'].map(encoding_station_dict)\n",
    "\n",
    "X_validation = X_validation[['station_encoded', 'precipitation', 'temperature_2m', 'altitude', 'capacity',\n",
    "                         'month', 'day','hour', 'CTX-1', 'CTX-2', 'CTX-3', 'CTX-4']]\n",
    "\n",
    "X_test['station_encoded'] = X_test['station_id'].map(encoding_station_dict)\n",
    "\n",
    "X_test = X_test[['station_encoded', 'precipitation', 'temperature_2m', 'altitude', 'capacity',\n",
    "                         'month', 'day','hour', 'CTX-1', 'CTX-2', 'CTX-3', 'CTX-4']]\n",
    "\n",
    "X_train_val['station_encoded'] = X_train_val['station_id'].map(encoding_station_dict)\n",
    "\n",
    "X_train_val = X_train_val[['station_encoded', 'precipitation', 'temperature_2m', 'altitude', 'capacity',\n",
    "                         'month', 'day','hour', 'CTX-1', 'CTX-2', 'CTX-3', 'CTX-4']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random split without 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T18:27:11.145319Z",
     "start_time": "2023-07-03T18:27:11.141598Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install category_encoders\n",
    "import category_encoders as ce\n",
    "\n",
    "X = clean_data[~clean_data['year'].isin([2020])].drop('percentage_docks_available', axis=1)  # Replace 'target_variable' with your actual target column name\n",
    "y = clean_data[~clean_data['year'].isin([2020])][['percentage_docks_available']]\n",
    "\n",
    "X_train_val_no_20, X_test_no_20, y_train_val_no_20, y_test_no_20 = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "X_control_no_20, X_validation_no_20, y_control_no_20, y_validation_no_20 = train_test_split(X_train_val_no_20, y_train_val_no_20, test_size=0.1/0.9, random_state=42)\n",
    "\n",
    "target_encoder = ce.TargetEncoder(cols=['station_id'])\n",
    "X_control_no_20['station_encoded'] = target_encoder.fit_transform(X_control_no_20['station_id'], y_control_no_20)\n",
    "\n",
    "encoding_station_dict = X_control_no_20.groupby(['station_id', 'station_encoded']).mean().reset_index()\\\n",
    "[['station_id', 'station_encoded']].set_index('station_id')['station_encoded'].to_dict()\n",
    "\n",
    "X_control_no_20 = X_control_no_20[['station_encoded', 'precipitation', 'temperature_2m', 'altitude', 'capacity',\n",
    "                         'month', 'day', 'hour','CTX-1', 'CTX-2', 'CTX-3', 'CTX-4']]\n",
    "\n",
    "X_validation_no_20['station_encoded'] = X_validation_no_20['station_id'].map(encoding_station_dict)\n",
    "\n",
    "X_validation_no_20 = X_validation_no_20[['station_encoded', 'precipitation', 'temperature_2m', 'altitude', 'capacity',\n",
    "                         'month', 'day','hour', 'CTX-1', 'CTX-2', 'CTX-3', 'CTX-4']]\n",
    "\n",
    "X_test_no_20['station_encoded'] = X_test_no_20['station_id'].map(encoding_station_dict)\n",
    "\n",
    "X_test_no_20 = X_test_no_20[['station_encoded', 'precipitation', 'temperature_2m', 'altitude', 'capacity',\n",
    "                         'month', 'day','hour', 'CTX-1', 'CTX-2', 'CTX-3', 'CTX-4']]\n",
    "\n",
    "X_train_val_no_20['station_encoded'] = X_train_val_no_20['station_id'].map(encoding_station_dict)\n",
    "\n",
    "X_train_val_no_20 = X_train_val_no_20[['station_encoded', 'precipitation', 'temperature_2m', 'altitude', 'capacity',\n",
    "                         'month', 'day','hour', 'CTX-1', 'CTX-2', 'CTX-3', 'CTX-4']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split for the Kaggle submission (not using 2023 data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T20:00:42.423500Z",
     "start_time": "2023-07-03T19:59:46.153303Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install category_encoders\n",
    "import category_encoders as ce\n",
    "\n",
    "X_train_no_23 = clean_data[~clean_data['year'].isin([2023])].drop('percentage_docks_available', axis=1)\n",
    "y_train_no_23 = clean_data[~clean_data['year'].isin([2023])][['percentage_docks_available']]\n",
    "\n",
    "target_encoder = ce.TargetEncoder(cols=['station_id'])\n",
    "X_train_no_23['station_encoded'] = target_encoder.fit_transform(X_train_no_23['station_id'], y_train_no_23)\n",
    "\n",
    "encoding_station_dict_no_23 = X_train_no_23.groupby(['station_id', 'station_encoded']).mean().reset_index()\\\n",
    "[['station_id', 'station_encoded']].set_index('station_id')['station_encoded'].to_dict()\n",
    "\n",
    "\n",
    "X_train_no_23=X_train_no_23[['station_encoded', 'precipitation', 'temperature_2m', 'altitude', 'capacity',\n",
    "                         'month', 'day','hour', 'CTX-1', 'CTX-2', 'CTX-3', 'CTX-4']]\n",
    "\n",
    "\n",
    "X_train_no_2023 = clean_data[~clean_data['year'].isin([2023,2020])].drop('percentage_docks_available', axis=1)\n",
    "y_train_no_2023 = clean_data[~clean_data['year'].isin([2023,2020])][['percentage_docks_available']]\n",
    "\n",
    "target_encoder = ce.TargetEncoder(cols=['station_id'])\n",
    "X_train_no_2023['station_encoded'] = target_encoder.fit_transform(X_train_no_2023['station_id'], y_train_no_2023)\n",
    "\n",
    "\n",
    "X_train_no_2023=X_train_no_2023[['station_encoded', 'precipitation', 'temperature_2m', 'altitude', 'capacity',\n",
    "                         'month', 'day','hour', 'CTX-1', 'CTX-2', 'CTX-3', 'CTX-4']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_validation_test_23 = clean_data[clean_data['year'].isin([2023])].drop('percentage_docks_available', axis=1)\n",
    "\n",
    "X_validation_test_23['station_encoded'] = X_validation_test_23['station_id'].map(encoding_station_dict_no_23)\n",
    "\n",
    "X_validation_test_23 = X_validation_test_23[['station_encoded', 'precipitation', 'temperature_2m', 'altitude', 'capacity',\n",
    "                         'month', 'day','hour', 'CTX-1', 'CTX-2', 'CTX-3', 'CTX-4']]\n",
    "\n",
    "y_validation_test_23 = clean_data[clean_data['year'].isin([2023])][['percentage_docks_available']]\n",
    "\n",
    "X_validation_23, X_test_23, y_validation_23, y_test_23 = train_test_split(X_validation_test_23,y_validation_test_23, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Withouth 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T18:29:11.710774Z",
     "start_time": "2023-07-03T18:29:11.708282Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import category_encoders as ce\n",
    "\n",
    "linear_reg = []\n",
    "\n",
    "# Create an instance of the LinearRegression model\n",
    "linear_reg.append(LinearRegression())\n",
    "linear_reg.append(LinearRegression())\n",
    "\n",
    "# Fit the model to the data\n",
    "linear_reg[0].fit(X_control, y_control)\n",
    "linear_reg[1].fit(X_control_no_20,y_control_no_20)\n",
    "\n",
    "# Print the coefficients and intercept\n",
    "print(\"Intercept:\", linear_reg[0].intercept_)\n",
    "print(\"Coefficients:\", linear_reg[0].coef_)\n",
    "print(\"Intercept:\", linear_reg[1].intercept_)\n",
    "print(\"Coefficients:\", linear_reg[1].coef_)\n",
    "print(\"Columns:\", X_control.columns)\n",
    "## El coeficiente de las precipitaciones es muy bajo, habrá que meter precipitación por hora\n",
    "## Pedir a ChatGPT fuente de datos históricos de precipitacion por hora en BCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T18:28:59.979260Z",
     "start_time": "2023-07-03T18:28:52.944Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "linear_reg_pred = []\n",
    "s = []\n",
    "mse = []\n",
    "desv = []\n",
    "\n",
    "s.append(linear_reg[0].score(X_control,y_control))\n",
    "s.append(linear_reg[1].score(X_control_no_20,y_control_no_20))\n",
    "\n",
    "# Make predictions on the validation data\n",
    "\n",
    "linear_reg_pred.append(linear_reg[0].predict(X_validation))\n",
    "linear_reg_pred.append(linear_reg[1].predict(X_validation_no_20))\n",
    "\n",
    "# Calculate the mean squared error (MSE) to evaluate the model's performance and its interval under 95% confidence level.\n",
    "\n",
    "mse.append(mean_squared_error(y_validation, linear_reg_pred[0]))\n",
    "mse.append(mean_squared_error(y_validation_no_20, linear_reg_pred[1]))\n",
    "\n",
    "desv.append(((y_validation[['percentage_docks_available']]-linear_reg_pred[0])*(y_validation[['percentage_docks_available']]-linear_reg_pred[0])).percentage_docks_available.std())\n",
    "desv.append(((y_validation_no_20[['percentage_docks_available']]-linear_reg_pred[1])*(y_validation_no_20[['percentage_docks_available']]-linear_reg_pred[1])).percentage_docks_available.std())\n",
    "\n",
    "print(\"Correlation Coefficient (R^2) is:\",s[0],s[1])\n",
    "print(\"MSE:\", mse[0],mse[1])\n",
    "print(\"MSE STD:\",desv[0],desv[1])\n",
    "print(\"Under 95% confidence level, the error is:\",mse[0]+1.96*desv[0],mse[1]+1.96*desv[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T18:29:54.763634Z",
     "start_time": "2023-07-03T18:29:54.760284Z"
    }
   },
   "source": [
    "##### Without 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T18:29:32.733819Z",
     "start_time": "2023-07-03T18:29:17.550202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [-0.0118362]\n",
      "Coefficients: [[ 1.67968971e-01 -9.17754404e-05  5.53054841e-04  8.33723347e-08\n",
      "   2.00824490e-06  4.54722499e-04 -2.31150702e-05 -6.42628890e-05\n",
      "   8.49270478e-01  9.04648726e-03 -7.71144394e-04 -2.55191903e-02]]\n",
      "Intercept: [-0.01192564]\n",
      "Coefficients: [[ 1.71129550e-01 -9.79586254e-04  6.05795945e-04  2.46845057e-06\n",
      "  -1.40477617e-06  3.16914730e-04 -1.22022322e-05 -6.01092093e-05\n",
      "   8.48049130e-01  9.12876816e-03 -5.63965009e-04 -2.79252051e-02]]\n",
      "Columns: Index(['station_encoded', 'precipitation', 'temperature_2m', 'altitude',\n",
      "       'capacity', 'month', 'day', 'hour', 'CTX-1', 'CTX-2', 'CTX-3', 'CTX-4'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import category_encoders as ce\n",
    "\n",
    "linear_reg = []\n",
    "\n",
    "# Create an instance of the LinearRegression model\n",
    "linear_reg.append(LinearRegression())\n",
    "linear_reg.append(LinearRegression())\n",
    "\n",
    "# Fit the model to the data\n",
    "linear_reg[0].fit(X_train_no_23, y_train_no_23)\n",
    "linear_reg[1].fit(X_train_no_2023,y_train_no_2023)\n",
    "\n",
    "# Print the coefficients and intercept\n",
    "print(\"Intercept:\", linear_reg[0].intercept_)\n",
    "print(\"Coefficients:\", linear_reg[0].coef_)\n",
    "print(\"Intercept:\", linear_reg[1].intercept_)\n",
    "print(\"Coefficients:\", linear_reg[1].coef_)\n",
    "print(\"Columns:\", X_control.columns)\n",
    "## El coeficiente de las precipitaciones es muy bajo, habrá que meter precipitación por hora\n",
    "## Pedir a ChatGPT fuente de datos históricos de precipitacion por hora en BCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T20:06:29.990886Z",
     "start_time": "2023-07-03T20:06:29.727514Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linear_reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-53b74e87f626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdesv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_reg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_no_23\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_no_23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_reg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_no_2023\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_no_2023\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'linear_reg' is not defined"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "linear_reg_pred = []\n",
    "s = []\n",
    "mse = []\n",
    "desv = []\n",
    "\n",
    "s.append(linear_reg[0].score(X_train_no_23,y_train_no_23))\n",
    "s.append(linear_reg[1].score(X_train_no_2023,y_train_no_2023))\n",
    "\n",
    "# Make predictions on the validation data\n",
    "\n",
    "linear_reg_pred.append(linear_reg[0].predict(X_validation_23))\n",
    "linear_reg_pred.append(linear_reg[1].predict(X_validation_23))\n",
    "\n",
    "# Calculate the mean squared error (MSE) to evaluate the model's performance and its interval under 95% confidence level.\n",
    "\n",
    "mse.append(mean_squared_error(y_validation_23, linear_reg_pred[0]))\n",
    "mse.append(mean_squared_error(y_validation_23, linear_reg_pred[1]))\n",
    "\n",
    "desv.append(((y_validation_23[['percentage_docks_available']]-linear_reg_pred[0])*(y_validation_23[['percentage_docks_available']]-linear_reg_pred[0])).percentage_docks_available.std())\n",
    "desv.append(((y_validation_23[['percentage_docks_available']]-linear_reg_pred[1])*(y_validation_23[['percentage_docks_available']]-linear_reg_pred[1])).percentage_docks_available.std())\n",
    "\n",
    "print(\"Correlation Coefficient (R^2) is:\",s[0],s[1])\n",
    "print(\"MSE:\", mse[0],mse[1])\n",
    "print(\"MSE STD:\",desv[0],desv[1])\n",
    "print(\"Under 95% confidence level, the error is:\",mse[0]+1.96*desv[0],mse[1]+1.96*desv[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest (parallelized for getting results quicker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T20:11:31.464869Z",
     "start_time": "2023-07-03T20:08:05.475285Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fernadavo/.virtualenvs/dev_new/lib/python3.7/site-packages/ipykernel_launcher.py:15: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  from ipykernel import kernelapp as app\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 50building tree 2 of 50\n",
      "building tree 3 of 50\n",
      "\n",
      "building tree 4 of 50\n",
      "building tree 5 of 50building tree 6 of 50building tree 7 of 50\n",
      "building tree 8 of 50\n",
      "\n",
      "\n",
      "building tree 9 of 50\n",
      "building tree 10 of 50\n",
      "building tree 11 of 50\n",
      "building tree 12 of 50\n",
      "building tree 13 of 50\n",
      "building tree 14 of 50\n",
      "building tree 15 of 50\n",
      "building tree 16 of 50\n",
      "building tree 17 of 50\n",
      "building tree 18 of 50\n",
      "building tree 19 of 50\n",
      "building tree 20 of 50\n",
      "building tree 21 of 50\n",
      "building tree 22 of 50\n",
      "building tree 23 of 50\n",
      "building tree 24 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   55.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 25 of 50\n",
      "building tree 26 of 50\n",
      "building tree 27 of 50\n",
      "building tree 28 of 50\n",
      "building tree 29 of 50\n",
      "building tree 30 of 50\n",
      "building tree 31 of 50\n",
      "building tree 32 of 50\n",
      "building tree 33 of 50\n",
      "building tree 34 of 50\n",
      "building tree 35 of 50\n",
      "building tree 36 of 50\n",
      "building tree 37 of 50\n",
      "building tree 38 of 50\n",
      "building tree 39 of 50\n",
      "building tree 40 of 50\n",
      "building tree 41 of 50\n",
      "building tree 42 of 50\n",
      "building tree 43 of 50\n",
      "building tree 44 of 50\n",
      "building tree 45 of 50\n",
      "building tree 46 of 50\n",
      "building tree 47 of 50\n",
      "building tree 48 of 50\n",
      "building tree 49 of 50\n",
      "building tree 50 of 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  3.2min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    4.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.023335484137738286]] [[0.05735448160035634, 0.05299813628763718, 0.049656744215125705]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  50 out of  50 | elapsed:   14.1s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "a = []\n",
    "b = []\n",
    "c = []\n",
    "d = []\n",
    "rf_model = []\n",
    "y_pred = []\n",
    "mse = []\n",
    "\n",
    "# rf_model.append(RandomForestRegressor(n_estimators=50,max_depth=5,random_state=42))\n",
    "# rf_model.append(RandomForestRegressor(n_estimators=100,max_depth=5,random_state=42))\n",
    "rf_model.append(RandomForestRegressor(n_estimators=50,random_state=42, verbose=3, n_jobs=-1))\n",
    "\n",
    "rf_model[0].fit(X_train_no_2023.head(1000000), y_train_no_2023.head(1000000))\n",
    "# rf_model[1].fit(X_train_no_2023, y_train_no_2023)\n",
    "# rf_model[2].fit(X_train_no_2023, y_train_no_2023)\n",
    "\n",
    "y_pred.append(rf_model[0].predict(X_validation_23))\n",
    "# y_pred.append(rf_model[1].predict(X_validation_23))\n",
    "#y_pred.append(rf_model[2].predict(X_validation_23))\n",
    "\n",
    "mse.append(mean_squared_error(y_validation_23, y_pred[0]))\n",
    "# mse.append(mean_squared_error(y_validation_23, y_pred[1]))\n",
    "#mse.append(mean_squared_error(y_validation_23, y_pred[2]))\n",
    "desv.append(((y_validation_23.percentage_docks_available-y_pred[0])*(y_validation_23.percentage_docks_available-y_pred[0])).std())\n",
    "# desv.append(((y_validation_23.percentage_docks_available-y_pred[1])*(y_validation_23.percentage_docks_available-y_pred[1])).std())\n",
    "#desv.append(((y_validation_23.percentage_docks_available-y_pred[2])*(y_validation_23.percentage_docks_available-y_pred[2])).std())\n",
    "a.append(mse)\n",
    "b.append(desv)\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T20:11:31.486018Z",
     "start_time": "2023-07-03T20:11:31.479708Z"
    }
   },
   "outputs": [],
   "source": [
    "# 43 seconds 50K rows -> Complete dataset in ~34 minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Train the Support Vector Machines (SVM) model\n",
    "svm_model = SVR()\n",
    "svm_model.fit(X_control_no_2023.head(20000), y_control.no_2023.head(20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Correlation Coefficient\n",
    "\n",
    "s = svm_model.score(X_train_no_2023.head(20000), y_train_no_2023.head(20000))\n",
    "\n",
    "# Make predictions on the validation data\n",
    "\n",
    "y_pred = svm_model.predict(X_validation_23)\n",
    "\n",
    "# Calculate the mean squared error (MSE) to evaluate the model's performance and its interval under 95% confidence level.\n",
    "\n",
    "mse = mean_squared_error(y_validation_23, y_pred)\n",
    "desv = ((y_validation_23.percentage_docks_available-y_pred)*(y_validation_23.percentage_docks_available-y_pred)).std()\n",
    "\n",
    "print(\"Correlation Coefficient (R^2) is:\",s)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"MSE STD:\",desv)\n",
    "print(\"Under 95% confidence level, the MSE is within the interval:\",mse+1.96*desv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 seconds 1K rows -> Complete dataset in ~5.4 hours (not manageble)\n",
    "len(X_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Scale the input features\n",
    "scaler = StandardScaler()\n",
    "X_control_scaled_no_23 = scaler.fit_transform(X_train_no_23.head(1000))\n",
    "X_control_scaled_no_2023 = scaler.fit_transform(X_train_no_2023.head(1000))\n",
    "\n",
    "X_validation_scaled_23 = scaler.transform(X_validation_23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1384\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13bfe68d0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN = []\n",
    "\n",
    "NN.append(Sequential())\n",
    "NN.append(Sequential())\n",
    "\n",
    "NN[0].add(Dense(64, activation='relu', input_shape=(X_control_scaled_no_23.shape[1],)))\n",
    "NN[1].add(Dense(64, activation='relu', input_shape=(X_control_scaled_no_2023.shape[1],)))\n",
    "\n",
    "NN[0].add(Dense(1, activation='linear'))\n",
    "NN[1].add(Dense(1, activation='linear'))\n",
    "\n",
    "NN[0].compile(loss='mean_squared_error', optimizer=Adam())\n",
    "NN[1].compile(loss='mean_squared_error', optimizer=Adam())\n",
    "\n",
    "NN[0].fit(X_control_scaled_no_23, np.array(y_train_no_23.head(1000)), epochs=1, batch_size=32, verbose=1)\n",
    "NN[1].fit(X_control_scaled_no_2023, np.array(y_train_no_2023.head(1000)), epochs=1, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 2ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "MSE: 6.738135625814013 12.082918574360603\n",
      "MSE STD: 6.340016297756087 26.75250562030755\n",
      "Under 95% confidence level, the error is: 19.164567569415944 64.5178295901634\n"
     ]
    }
   ],
   "source": [
    "#s.append(NN[0].score(X_control_scaled_no_23,y_train_no_23))\n",
    "#s.append(NN[1].score(X_control_scaled_no_2023,y_train_no_2023))\n",
    "\n",
    "X_validation_scaled_23 = scaler.transform(X_validation_23)\n",
    "# Make predictions on the validation data\n",
    "NN_pred = []\n",
    "mse = []\n",
    "desv = []\n",
    "\n",
    "NN_pred.append(NN[0].predict(X_validation_scaled_23))\n",
    "NN_pred.append(NN[1].predict(X_validation_scaled_23))\n",
    "\n",
    "# Calculate the mean squared error (MSE) to evaluate the model's performance and its interval under 95% confidence level.\n",
    "\n",
    "mse.append(mean_squared_error(np.array(y_validation_23), NN_pred[0]))\n",
    "mse.append(mean_squared_error(np.array(y_validation_23), NN_pred[1]))\n",
    "\n",
    "desv.append(((y_validation_23[['percentage_docks_available']]-NN_pred[0])*(y_validation_23[['percentage_docks_available']]-NN_pred[0])).percentage_docks_available.std())\n",
    "desv.append(((y_validation_23[['percentage_docks_available']]-NN_pred[1])*(y_validation_23[['percentage_docks_available']]-NN_pred[1])).percentage_docks_available.std())\n",
    "\n",
    "# print(\"Correlation Coefficient (R^2) is:\",s[0],s[1])\n",
    "print(\"MSE:\", mse[0],mse[1])\n",
    "print(\"MSE STD:\",desv[0],desv[1])\n",
    "print(\"Under 95% confidence level, the error is:\",mse[0]+1.96*desv[0],mse[1]+1.96*desv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T16:42:33.808246Z",
     "start_time": "2023-07-03T16:42:33.804567Z"
    }
   },
   "outputs": [],
   "source": [
    "# desv.append(((y_validation_23[['percentage_docks_available']]-NN_pred[0])*(y_validation_23[['percentage_docks_available']]-NN_pred[0])).percentage_docks_available.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "339553/339553 [==============================] - 459s 1ms/step - loss: 0.0204\n",
      "Epoch 2/3\n",
      "339553/339553 [==============================] - 420s 1ms/step - loss: 0.0201\n",
      "Epoch 3/3\n",
      "339553/339553 [==============================] - 428s 1ms/step - loss: 0.0201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [21:48<1:27:14, 1308.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "339553/339553 [==============================] - 464s 1ms/step - loss: 0.0200\n",
      "Epoch 2/3\n",
      "339553/339553 [==============================] - 463s 1ms/step - loss: 0.0197\n",
      "Epoch 3/3\n",
      "339553/339553 [==============================] - 464s 1ms/step - loss: 0.0196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [45:00<1:07:53, 1357.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "339553/339553 [==============================] - 499s 1ms/step - loss: 0.0200\n",
      "Epoch 2/3\n",
      "339553/339553 [==============================] - 505s 1ms/step - loss: 0.0196\n",
      "Epoch 3/3\n",
      "339553/339553 [==============================] - 508s 1ms/step - loss: 0.0195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [1:10:13<47:37, 1428.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "339553/339553 [==============================] - 530s 2ms/step - loss: 0.0200\n",
      "Epoch 2/3\n",
      "339553/339553 [==============================] - 529s 2ms/step - loss: 0.0196\n",
      "Epoch 3/3\n",
      "339553/339553 [==============================] - 531s 2ms/step - loss: 0.0195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [1:36:45<24:52, 1492.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "339553/339553 [==============================] - 592s 2ms/step - loss: 0.0200\n",
      "Epoch 2/3\n",
      "339553/339553 [==============================] - 588s 2ms/step - loss: 0.0196\n",
      "Epoch 3/3\n",
      "339553/339553 [==============================] - 566s 2ms/step - loss: 0.0195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [2:05:52<00:00, 1510.52s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Scale the input features\n",
    "scaler = StandardScaler()\n",
    "X_control_scaled_no_23 = scaler.fit_transform(X_train_no_23)\n",
    "\n",
    "X_control_scaled_no_2023 = scaler.fit_transform(X_train_no_2023)\n",
    "\n",
    "X_validation_scaled_23 = scaler.transform(X_validation_23)\n",
    "\n",
    "NNN = []\n",
    "\n",
    "for i in tqdm(range(0,5)):\n",
    "    NNN.append(Sequential())\n",
    "    for j in range(0,i+1):\n",
    "        NNN[i].add(Dense(64/(j+1), activation='relu', input_shape=(X_control_scaled_no_2023.shape[1],)))\n",
    "    NNN[i].add(Dense(1, activation='sigmoid'))\n",
    "    NNN[i].compile(loss='mean_squared_error', optimizer=Adam())\n",
    "    NNN[i].fit(X_control_scaled_no_2023, np.array(y_train_no_2023), epochs=3, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "339553/339553 [==============================] - 667s 2ms/step - loss: 0.0202\n",
      "Epoch 2/3\n",
      "339553/339553 [==============================] - 675s 2ms/step - loss: 0.0198\n",
      "Epoch 3/3\n",
      "339553/339553 [==============================] - 680s 2ms/step - loss: 0.0197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [33:45<2:15:00, 2025.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "339553/339553 [==============================] - 577s 2ms/step - loss: 0.0201\n",
      "Epoch 2/3\n",
      "339553/339553 [==============================] - 584s 2ms/step - loss: 0.0197\n",
      "Epoch 3/3\n",
      "339553/339553 [==============================] - 585s 2ms/step - loss: 0.0196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [1:02:51<1:33:03, 1861.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "339553/339553 [==============================] - 590s 2ms/step - loss: 0.0201\n",
      "Epoch 2/3\n",
      "339553/339553 [==============================] - 594s 2ms/step - loss: 0.0197\n",
      "Epoch 3/3\n",
      "339553/339553 [==============================] - 605s 2ms/step - loss: 0.0196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [1:32:42<1:00:57, 1828.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "339553/339553 [==============================] - 598s 2ms/step - loss: 0.0201\n",
      "Epoch 2/3\n",
      "339553/339553 [==============================] - 604s 2ms/step - loss: 0.0197\n",
      "Epoch 3/3\n",
      "339553/339553 [==============================] - 604s 2ms/step - loss: 0.0196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [2:02:50<30:20, 1820.66s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "339553/339553 [==============================] - 605s 2ms/step - loss: 0.0201\n",
      "Epoch 2/3\n",
      "339553/339553 [==============================] - 608s 2ms/step - loss: 0.0197\n",
      "Epoch 3/3\n",
      "339553/339553 [==============================] - 610s 2ms/step - loss: 0.0196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [2:33:13<00:00, 1838.67s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5,10)):\n",
    "    NNN.append(Sequential())\n",
    "    for j in range(0,i+1):\n",
    "        NNN[i].add(Dense(64/(j+1), activation='relu', input_shape=(X_control_scaled_no_2023.shape[1],)))\n",
    "    NNN[i].add(Dense(1, activation='sigmoid'))\n",
    "    NNN[i].compile(loss='mean_squared_error', optimizer=Adam())\n",
    "    NNN[i].fit(X_control_scaled_no_2023, np.array(y_train_no_2023), epochs=3, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20910/20910 [==============================] - 23s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [00:28<01:53, 28.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20910/20910 [==============================] - 25s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [00:57<01:27, 29.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20910/20910 [==============================] - 25s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [01:27<00:58, 29.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20910/20910 [==============================] - 25s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [01:57<00:29, 29.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20910/20910 [==============================] - 26s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:28<00:00, 29.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: [0.019603723044956597, 0.019297837915115572, 0.019108578633197416, 0.019055276096905487, 0.019091852482526598]\n",
      "MSE STD: [0.052420569211507395, 0.05371747782656764, 0.05058249123966113, 0.052340100005496194, 0.05021551950727971]\n",
      "Under 95% confidence level, the error is: [0.019603723044956597, 0.019297837915115572, 0.019108578633197416, 0.019055276096905487, 0.019091852482526598, 0.052420569211507395, 0.05371747782656764, 0.05058249123966113, 0.052340100005496194, 0.05021551950727971, 0.052420569211507395, 0.05371747782656764, 0.05058249123966113, 0.052340100005496194, 0.05021551950727971]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NNN_pred = []\n",
    "mse = []\n",
    "desv = []\n",
    "\n",
    "X_validation_scaled_23 = scaler.transform(X_validation_23)\n",
    "\n",
    "for i in tqdm(range(0,5)):\n",
    "    NNN_pred.append(NNN[i].predict(X_validation_scaled_23))\n",
    "    mse.append(mean_squared_error(np.array(y_validation_23),NNN_pred[i]))\n",
    "    desv.append(((y_validation_23[['percentage_docks_available']]-NNN_pred[i])*(y_validation_23[['percentage_docks_available']]-NNN_pred[i])).percentage_docks_available.std())\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"MSE STD:\",desv)\n",
    "#print(\"Under 95% confidence level, the error is:\",mse+2*desv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T20:18:01.229134Z",
     "start_time": "2023-07-03T20:18:00.620388Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('metadata_sample_submission.csv')\n",
    "test= test.merge(info,on='station_id',how='left')\n",
    "test['station_encoded'] = test['station_id'].map(encoding_station_dict_no_23)\n",
    "test['year']='2023'\n",
    "test['date'] = pd.to_datetime(test[['year','month', 'day']])\n",
    "test['date']=test['date'].astype(str)\n",
    "test = test.merge(weather_data[['date','hour', 'precipitation', 'temperature_2m']] ,on=['date','hour'], how='left')\n",
    "test = test.rename(columns={'ctx-1': 'CTX-1', 'ctx-2': 'CTX-2','ctx-3':'CTX-3','ctx-4':'CTX-4'})\n",
    "test=test[['station_encoded', 'precipitation', 'temperature_2m', 'altitude',\n",
    "      'capacity', 'month', 'day', 'hour', 'CTX-1', 'CTX-2', 'CTX-3', 'CTX-4']]\n",
    "\n",
    "test_scaled_kaggle = scaler.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T20:18:19.711200Z",
     "start_time": "2023-07-03T20:18:19.072087Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done  50 out of  50 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "aux = NN.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-03T20:18:24.067973Z",
     "start_time": "2023-07-03T20:18:23.918388Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(aux,columns=['percentage_docks_available']).to_csv('kaggle_test_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dC_K9wWJHDEo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
